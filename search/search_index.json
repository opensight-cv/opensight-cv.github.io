{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenSight: the powerful, easy-to-use vision suite \u00b6 OpenSight is an FRC-focused, free and open source computer vision system targeted specifically for the Raspberry Pi. Our goal is to make it easy for people not familiar with vision to be able to make complex pipelines, while also providing powerful functionality for advanced users. Click here for information on how to install OpenSight! \u00b6 About \u00b6 Our mission is to create an accessible vision suite, with an easy-to-use and works out-of-the box experience, but also allow for more power and greater customizability. We want to make vision more accessible to those with less experience, while also providing the tools for power users and developers to easily add features beyond the default modules. Have any questions, comments, or want to contribute? \u00b6 You can learn more about contributing here ! TL;DR: Join the OpenSight Discord server and we'll get you set up!","title":"Home"},{"location":"#opensight-the-powerful-easy-to-use-vision-suite","text":"OpenSight is an FRC-focused, free and open source computer vision system targeted specifically for the Raspberry Pi. Our goal is to make it easy for people not familiar with vision to be able to make complex pipelines, while also providing powerful functionality for advanced users.","title":"OpenSight: the powerful, easy-to-use vision suite"},{"location":"#click-here-for-information-on-how-to-install-opensight","text":"","title":"Click here for information on how to install OpenSight!"},{"location":"#about","text":"Our mission is to create an accessible vision suite, with an easy-to-use and works out-of-the box experience, but also allow for more power and greater customizability. We want to make vision more accessible to those with less experience, while also providing the tools for power users and developers to easily add features beyond the default modules.","title":"About"},{"location":"#have-any-questions-comments-or-want-to-contribute","text":"You can learn more about contributing here ! TL;DR: Join the OpenSight Discord server and we'll get you set up!","title":"Have any questions, comments, or want to contribute?"},{"location":"planned/","text":"Planned pages \u00b6 Modules: Contours: modules/contours.md Video IO: modules/videoio.md Color Operations: modules/color.md Mask Operations: modules/mask.md Drawing Operations: modules/draw.md NetworkTables: modules/nt.md","title":"Planned Pages"},{"location":"planned/#planned-pages","text":"Modules: Contours: modules/contours.md Video IO: modules/videoio.md Color Operations: modules/color.md Mask Operations: modules/mask.md Drawing Operations: modules/draw.md NetworkTables: modules/nt.md","title":"Planned pages"},{"location":"roadmap/","text":"OpenSight is still very much an active project. You can see our Trello board for a full list of everything we plan to do in the near future. Here's some of the highlights: Contour Filtering functions (such as area, aspect ratio, fullness, count) Static IP networking settings H.264 CameraServer with Shuffleboard plugin Basic conditional logic functions GPIO control module If you would like to help out with any of these features or you have any feedback whatsoever, please join our Discord ! We would love to hear your feedback in the #feedback channel. If you are interested in contributing, say so and any preference as to what to work on in #general and we will get you set up as soon as possible.","title":"Roadmap and Contributing"},{"location":"module-dev/basic-template/","text":"Basic Template \u00b6 Here is a basic module template you can you to get started: from dataclasses import dataclass from opsi.manager.manager_schema import Function from opsi.util.cv.mat import Mat __package__ = \"opsi.MODULE_NAME\" __version__ = \"0.XXX\" class FUNCTION_NAME ( Function ): @dataclass class Settings : name : type @dataclass class Inputs : name : type @dataclass class Outputs : name : type def run ( self , inputs ): img = inputs . img return self . Outputs ( img = img )","title":"Basic Template"},{"location":"module-dev/basic-template/#basic-template","text":"Here is a basic module template you can you to get started: from dataclasses import dataclass from opsi.manager.manager_schema import Function from opsi.util.cv.mat import Mat __package__ = \"opsi.MODULE_NAME\" __version__ = \"0.XXX\" class FUNCTION_NAME ( Function ): @dataclass class Settings : name : type @dataclass class Inputs : name : type @dataclass class Outputs : name : type def run ( self , inputs ): img = inputs . img return self . Outputs ( img = img )","title":"Basic Template"},{"location":"module-dev/glossary/","text":"Glossary \u00b6 Work in progress. \u00b6 This page contains a list of the words you may seen used in these documents or in disucssion. Module A directory or single Python file which supplies OpenSight with Functions.","title":"Glossary"},{"location":"module-dev/glossary/#glossary","text":"","title":"Glossary"},{"location":"module-dev/glossary/#work-in-progress","text":"This page contains a list of the words you may seen used in these documents or in disucssion. Module A directory or single Python file which supplies OpenSight with Functions.","title":"Work in progress."},{"location":"module-dev/intro/","text":"Creating Modules \u00b6 Modules make up a great portion of OpenSight's functionality. They are special types of classes that can operate on inputs and give outputs. They can be used in a wide variety of ways, such as modifing an image, performing math operations, or communicating with a server. Here's an example module, which does the following: Has a setting for the radius of the blur (of type integer) Inputs an image (of type Mat) Outputs the blurred image from dataclasses import dataclass from opsi.manager.manager_schema import Function from opsi.util.cv.mat import Mat , MatBW __package__ = \"opsi.example_module\" __version__ = \"0.123\" class Blur ( Function ): @dataclass class Settings : radius : int @dataclass class Inputs : img : Mat @dataclass class Outputs : img : Mat def run ( self , inputs ): img = cvw . blur ( inputs . img , self . settings . radius ) return self . Outputs ( img = img ) There are already many useful modules included in OpenSight, but creating your own can add more complexity to your vision pipeline. The source code for the modules is located under opsi/modules in the OpenSight repo. Warning Using the cv2 library directly is discouraged. All new OpenCV functionality should go into cvwrapper.py .","title":"Introduction"},{"location":"module-dev/intro/#creating-modules","text":"Modules make up a great portion of OpenSight's functionality. They are special types of classes that can operate on inputs and give outputs. They can be used in a wide variety of ways, such as modifing an image, performing math operations, or communicating with a server. Here's an example module, which does the following: Has a setting for the radius of the blur (of type integer) Inputs an image (of type Mat) Outputs the blurred image from dataclasses import dataclass from opsi.manager.manager_schema import Function from opsi.util.cv.mat import Mat , MatBW __package__ = \"opsi.example_module\" __version__ = \"0.123\" class Blur ( Function ): @dataclass class Settings : radius : int @dataclass class Inputs : img : Mat @dataclass class Outputs : img : Mat def run ( self , inputs ): img = cvw . blur ( inputs . img , self . settings . radius ) return self . Outputs ( img = img ) There are already many useful modules included in OpenSight, but creating your own can add more complexity to your vision pipeline. The source code for the modules is located under opsi/modules in the OpenSight repo. Warning Using the cv2 library directly is discouraged. All new OpenCV functionality should go into cvwrapper.py .","title":"Creating Modules"},{"location":"module-dev/structure/","text":"Module Structure \u00b6 Modules \u00b6 TODO: explain module package structure Functions (break into separate file?) \u00b6 TODO: explain general class setup A function has three main user-facing components: Inputs , Outputs , and Settings . Each one of these is a statically defined Python dataclass . You can create a dataclass by creating a class with the @dataclass decorator, as shown here: @dataclass class Inputs : x : int","title":"Module Structure"},{"location":"module-dev/structure/#module-structure","text":"","title":"Module Structure"},{"location":"module-dev/structure/#modules","text":"TODO: explain module package structure","title":"Modules"},{"location":"module-dev/structure/#functions-break-into-separate-file","text":"TODO: explain general class setup A function has three main user-facing components: Inputs , Outputs , and Settings . Each one of these is a statically defined Python dataclass . You can create a dataclass by creating a class with the @dataclass decorator, as shown here: @dataclass class Inputs : x : int","title":"Functions (break into separate file?)"},{"location":"modules/color/","text":"The ColorOps module contains nodes used to manipulate an image directly. These nodes can be found under the opsi-colorops tab. Absolute Difference RGB/HSV \u00b6 The Absolute Difference node takes an input img and calculates the difference between each pixel in the image and a reference value. The RGB node has a red , green , and blue parameter to set the reference color. If a pixel in the input exactly matches the reference, the pixel in the output will be black. The further the pixel is from the reference, the brighter the output pixel will be. The RGB node also has a to_greyscale parameter. When to_greyscale is off, the output will be in RGB with the each channel being set to the difference from the reference in that channel. This can be used to invert an image by setting to_greyscale to false and set red , green , and blue to 255. The HSV node has a hue , sat , and val parameter to set the reference color in the HSV color system. HSV is a color system that reflects how humans perceive color. More information on the HSV color system can be found in this article . There are also sensitivity settings that effect how sensitive the output is to differences in hue, saturation, and value individually. For instance, if you only care about the hue of a pixel, you can set hue_sensitivity to some positive value and sat_sensitivity and val_sensitivity to zero. Both the RGB and HSV nodes have a clamp_max setting. If clamp_max is selected, any pixels with a value above clamp_value will have their value set to clamp_value . This can be used to do a form of thresholding where pixels that match the reference color appear dark (But still have some contrast) and everything else is a flat background. Blur \u00b6 The Blur node takes an input img and blurs the image to produce the output img . The blur process is known as a box blur, where each pixel's red, green, and blue values are individually averaged with the pixels surrounding it. The number of pixels it is averaged with determined by the radius setting, which is the radius of the surrounding pixels to be averaged with. Canny \u00b6 The Canny node takes an input img and detects edges in the image. The output is a black-and-white image with white pixels representing lines in the input image. The Canny node has a threshold value to set its sensitivity. Any edge that is stronger than the upper end of the threshold setting is automatically kept, and any edge between the lower and upper threshold is only kept if it is connected to an edge above the upper edge of the threshold. You can read more about the threshold setting in this OpenCV tutorial . Clamp Min/Max \u00b6 The clamp min/max nodes limit the minimum and maximum value of pixels in an image. ClampMin increases the brightness of any pixel below its threshold to its threshold. ClampMax decreases the brightnss of any pixel above its threshold to its threshold. Color Sampler \u00b6 The Color Sampler node samples the color of a point in an image and outputs the result in RGB. It has two sliders, x_pct and y_pct , which are used to set the point in the image that is sampled. These values are percentages of the image resolution. There is also an option draw_color to draw a point on the image and the RGB values detected, or the HSV values detected by selecting the draw_hsv option. Color Detector \u00b6 The Color Detector node is intended to be used with the Color Sampler node to detect which color of the Control Panel from Infinite Recharge is being sampled. The node has a parameter for the hue of each color of the color wheel. You can tune these values by enabling the draw_color and draw_hsv options on the Color Sampler node to view the HSV values of each section of the color wheel. Greyscale \u00b6 The Greyscale node takes an input img and outputs the greyscale version of that image to img . This is done by multiplying each pixel's red, green, and blue values by constant value, then summing them, to produce a greyscale image. Each red value is multiplied by 0.299, green by 0.587, blue by 0.114. These constants are based off of how humans perceive color. Note The output of this node is not an imgBW , but is instead a normal img that only contains greyscale values, and thus cannot be used as an input for nodes such as the FindContours node. HSVRange \u00b6 The HSVRange node takes an input img and outputs a black and white image to imgBW . This image's white pixels are pixels whose color passed the HSV filter described in the node's settings. Each setting contains a slider with two controllable handles, which define the upper and lower acceptable bounds of each value. HSV is a color system that reflects how humans perceive color. More information on the HSV color system can be found in this article .","title":"Color Operations"},{"location":"modules/color/#absolute-difference-rgbhsv","text":"The Absolute Difference node takes an input img and calculates the difference between each pixel in the image and a reference value. The RGB node has a red , green , and blue parameter to set the reference color. If a pixel in the input exactly matches the reference, the pixel in the output will be black. The further the pixel is from the reference, the brighter the output pixel will be. The RGB node also has a to_greyscale parameter. When to_greyscale is off, the output will be in RGB with the each channel being set to the difference from the reference in that channel. This can be used to invert an image by setting to_greyscale to false and set red , green , and blue to 255. The HSV node has a hue , sat , and val parameter to set the reference color in the HSV color system. HSV is a color system that reflects how humans perceive color. More information on the HSV color system can be found in this article . There are also sensitivity settings that effect how sensitive the output is to differences in hue, saturation, and value individually. For instance, if you only care about the hue of a pixel, you can set hue_sensitivity to some positive value and sat_sensitivity and val_sensitivity to zero. Both the RGB and HSV nodes have a clamp_max setting. If clamp_max is selected, any pixels with a value above clamp_value will have their value set to clamp_value . This can be used to do a form of thresholding where pixels that match the reference color appear dark (But still have some contrast) and everything else is a flat background.","title":"Absolute Difference RGB/HSV"},{"location":"modules/color/#blur","text":"The Blur node takes an input img and blurs the image to produce the output img . The blur process is known as a box blur, where each pixel's red, green, and blue values are individually averaged with the pixels surrounding it. The number of pixels it is averaged with determined by the radius setting, which is the radius of the surrounding pixels to be averaged with.","title":"Blur"},{"location":"modules/color/#canny","text":"The Canny node takes an input img and detects edges in the image. The output is a black-and-white image with white pixels representing lines in the input image. The Canny node has a threshold value to set its sensitivity. Any edge that is stronger than the upper end of the threshold setting is automatically kept, and any edge between the lower and upper threshold is only kept if it is connected to an edge above the upper edge of the threshold. You can read more about the threshold setting in this OpenCV tutorial .","title":"Canny"},{"location":"modules/color/#clamp-minmax","text":"The clamp min/max nodes limit the minimum and maximum value of pixels in an image. ClampMin increases the brightness of any pixel below its threshold to its threshold. ClampMax decreases the brightnss of any pixel above its threshold to its threshold.","title":"Clamp Min/Max"},{"location":"modules/color/#color-sampler","text":"The Color Sampler node samples the color of a point in an image and outputs the result in RGB. It has two sliders, x_pct and y_pct , which are used to set the point in the image that is sampled. These values are percentages of the image resolution. There is also an option draw_color to draw a point on the image and the RGB values detected, or the HSV values detected by selecting the draw_hsv option.","title":"Color Sampler"},{"location":"modules/color/#color-detector","text":"The Color Detector node is intended to be used with the Color Sampler node to detect which color of the Control Panel from Infinite Recharge is being sampled. The node has a parameter for the hue of each color of the color wheel. You can tune these values by enabling the draw_color and draw_hsv options on the Color Sampler node to view the HSV values of each section of the color wheel.","title":"Color Detector"},{"location":"modules/color/#greyscale","text":"The Greyscale node takes an input img and outputs the greyscale version of that image to img . This is done by multiplying each pixel's red, green, and blue values by constant value, then summing them, to produce a greyscale image. Each red value is multiplied by 0.299, green by 0.587, blue by 0.114. These constants are based off of how humans perceive color. Note The output of this node is not an imgBW , but is instead a normal img that only contains greyscale values, and thus cannot be used as an input for nodes such as the FindContours node.","title":"Greyscale"},{"location":"modules/color/#hsvrange","text":"The HSVRange node takes an input img and outputs a black and white image to imgBW . This image's white pixels are pixels whose color passed the HSV filter described in the node's settings. Each setting contains a slider with two controllable handles, which define the upper and lower acceptable bounds of each value. HSV is a color system that reflects how humans perceive color. More information on the HSV color system can be found in this article .","title":"HSVRange"},{"location":"modules/contourfilters/","text":"The Contour Filter module contains nodes used to filter out unwanted contours found by FindContours. These nodes can be found under the opsi-contour-filter-ops tab. Typically, you will want to use a chain of contour filters ending with a Sort node like this: AngleFilter \u00b6 Filters contours based on the area of their minimum area bounding rectangle. Any contour whose minimum area rectangle is not within the range of the angle parameter will be filtered out. An angle of zero means that the top/bottom side of the rectangle will be horizontal. Since this node makes no distinction between horizontal and vertical rectangles, it is best to use it in conjuction with the OrientationFilter node. AreaFilter \u00b6 Filters contours based on the percentage of the area of the image they cover. Any contour whose area as a percentage of the image is not between min_area_pct and max_area_pct will be filtered out. Since this node uses area as a percentage of the image, it should act the same regardless of image resolution. AspectRatioFilter \u00b6 Filters contours based on their aspect ratio. The aspect ratio of a contour is defined as the ratio between the length of the longest and shortest side of its minimum area rectangle. Any contour whose aspect ratio is not between aspect_ratio_min and aspect_ratio_max will be filtered out. BoundingRectFilter & MinRectFilter \u00b6 Filters contours based on how close to a rectangle their shape is. This is defined by the percentage of their bounding rectangle they cover. Any contour whose area as a percentage of the area of its bounding rectangle is not within the given range will be filtered out. In the case of BoundingRectFilter, the countours' non-rotated bounding rectangle is used, and in the case of MinRectFilter, their rotated minimum area rectangle is used. OrientationFilter \u00b6 Filters contours based on their orientation. Contours that are shorter than they are wide are considered horizontal, and contours that are taller than they are wide are considered vertical. Only contours matching the orientation setting will be kept. SpeckleFilter \u00b6 Filters out contours that are much smaller than the largest contour. Any contour whose area is less than min_relative_area % of the largest contour's area is filtered out. This is useful when there are multiple targets that are close to the same size. Sort \u00b6 The sort node orders the contours by either their area or their location in the image and optionally keeps the top contour or the top several contours in the sorted list. The contours are ordered by the criteria selected for the by parameter. Top , Bottom , Left , Right , and Center sort the contours based on their position in the image. Largest and Smallest sort the contours based on their area. The keep parameter determines how many contours are kept. If keep is set to One , only the top contour in the sorted list is kept. If keep is set to Number , the top keep_amount of contours are kept. If keep is set to All , all contours are kept, but they are still sorted. keep_amount parameter is only used when keep is set to Number .","title":"Contourfilters"},{"location":"modules/contourfilters/#anglefilter","text":"Filters contours based on the area of their minimum area bounding rectangle. Any contour whose minimum area rectangle is not within the range of the angle parameter will be filtered out. An angle of zero means that the top/bottom side of the rectangle will be horizontal. Since this node makes no distinction between horizontal and vertical rectangles, it is best to use it in conjuction with the OrientationFilter node.","title":"AngleFilter"},{"location":"modules/contourfilters/#areafilter","text":"Filters contours based on the percentage of the area of the image they cover. Any contour whose area as a percentage of the image is not between min_area_pct and max_area_pct will be filtered out. Since this node uses area as a percentage of the image, it should act the same regardless of image resolution.","title":"AreaFilter"},{"location":"modules/contourfilters/#aspectratiofilter","text":"Filters contours based on their aspect ratio. The aspect ratio of a contour is defined as the ratio between the length of the longest and shortest side of its minimum area rectangle. Any contour whose aspect ratio is not between aspect_ratio_min and aspect_ratio_max will be filtered out.","title":"AspectRatioFilter"},{"location":"modules/contourfilters/#boundingrectfilter-minrectfilter","text":"Filters contours based on how close to a rectangle their shape is. This is defined by the percentage of their bounding rectangle they cover. Any contour whose area as a percentage of the area of its bounding rectangle is not within the given range will be filtered out. In the case of BoundingRectFilter, the countours' non-rotated bounding rectangle is used, and in the case of MinRectFilter, their rotated minimum area rectangle is used.","title":"BoundingRectFilter &amp; MinRectFilter"},{"location":"modules/contourfilters/#orientationfilter","text":"Filters contours based on their orientation. Contours that are shorter than they are wide are considered horizontal, and contours that are taller than they are wide are considered vertical. Only contours matching the orientation setting will be kept.","title":"OrientationFilter"},{"location":"modules/contourfilters/#specklefilter","text":"Filters out contours that are much smaller than the largest contour. Any contour whose area is less than min_relative_area % of the largest contour's area is filtered out. This is useful when there are multiple targets that are close to the same size.","title":"SpeckleFilter"},{"location":"modules/contourfilters/#sort","text":"The sort node orders the contours by either their area or their location in the image and optionally keeps the top contour or the top several contours in the sorted list. The contours are ordered by the criteria selected for the by parameter. Top , Bottom , Left , Right , and Center sort the contours based on their position in the image. Largest and Smallest sort the contours based on their area. The keep parameter determines how many contours are kept. If keep is set to One , only the top contour in the sorted list is kept. If keep is set to Number , the top keep_amount of contours are kept. If keep is set to All , all contours are kept, but they are still sorted. keep_amount parameter is only used when keep is set to Number .","title":"Sort"},{"location":"modules/contours/","text":"The Contours module contains nodes used to find and manipulate contours in an image. These nodes can be found under the opsi-contours tab. FindContours \u00b6 Find and outputs the contours of the input imgBW . This utilizes the OpenCV method findContours to find all of the contours in the image. A contour is the set of points that encircles each section of the input image that is white (meaning that those areas were passed a threshold of color). This node is the root source of all contours for nodes such as FindCenter of ConvexHulls. ConvexHulls \u00b6 Finds and outputs the convex hull of each contour in the input contours . A convex hull is the convex form of a contour, meaning that the outside angles of each vertex is at least 180 degrees. One way to make this easier to understand is to imagine wrapping a string around the contour. This string would be the convex hull, as all its outside angles would be greater than 180 degrees. ContourApproximate \u00b6 This node approximates contours by reducing their number of points. It takes one perameter, epsilon , which is the maximum distance between the output contour and the input contour. This node works the same way as the OpenCV function ApproxPolyDP . FindCenter \u00b6 Finds and outputs the center of the contours supplied. The node starts by finding the center of each individual contour. This is done by finding the bounding box of each contour and finding the center of that bounding box. Next, the overall center is determined by taking the center of each contour and finding the center of those points. The output center is this final center that is determined, scaled from -1 to 1 in each axis, where (0,0) is the center of the image. This scaling makes changing cameras or resolution much easier. The output success is True if at least one contour is present to have its center found, and is otherwise False . The draw setting is whether to draw the contours onto the img input before outputting them. If the setting is enabled, the bounding boxes, centers, and connecting lines will be drawn on the image before being output to visual . If the setting is disabled, then the output visual will simply be the input img . FindAngle \u00b6 This node converts coordinates (Like those found by FindCenter) to radians based on the resolution of the image and the FOV of the camera.","title":"Contours"},{"location":"modules/contours/#findcontours","text":"Find and outputs the contours of the input imgBW . This utilizes the OpenCV method findContours to find all of the contours in the image. A contour is the set of points that encircles each section of the input image that is white (meaning that those areas were passed a threshold of color). This node is the root source of all contours for nodes such as FindCenter of ConvexHulls.","title":"FindContours"},{"location":"modules/contours/#convexhulls","text":"Finds and outputs the convex hull of each contour in the input contours . A convex hull is the convex form of a contour, meaning that the outside angles of each vertex is at least 180 degrees. One way to make this easier to understand is to imagine wrapping a string around the contour. This string would be the convex hull, as all its outside angles would be greater than 180 degrees.","title":"ConvexHulls"},{"location":"modules/contours/#contourapproximate","text":"This node approximates contours by reducing their number of points. It takes one perameter, epsilon , which is the maximum distance between the output contour and the input contour. This node works the same way as the OpenCV function ApproxPolyDP .","title":"ContourApproximate"},{"location":"modules/contours/#findcenter","text":"Finds and outputs the center of the contours supplied. The node starts by finding the center of each individual contour. This is done by finding the bounding box of each contour and finding the center of that bounding box. Next, the overall center is determined by taking the center of each contour and finding the center of those points. The output center is this final center that is determined, scaled from -1 to 1 in each axis, where (0,0) is the center of the image. This scaling makes changing cameras or resolution much easier. The output success is True if at least one contour is present to have its center found, and is otherwise False . The draw setting is whether to draw the contours onto the img input before outputting them. If the setting is enabled, the bounding boxes, centers, and connecting lines will be drawn on the image before being output to visual . If the setting is disabled, then the output visual will simply be the input img .","title":"FindCenter"},{"location":"modules/contours/#findangle","text":"This node converts coordinates (Like those found by FindCenter) to radians based on the resolution of the image and the FOV of the camera.","title":"FindAngle"},{"location":"modules/draw/","text":"The Draw module contains nodes used to draw on images. These nodes can be found under the opsi-draw tab. BitwiseAND \u00b6 Uses a mask to select and output only the pixels of the input img that are white on the mask input. These pixels are taken and put to an output img , with the rest of the pixels being black. DrawContours \u00b6 Takes an image img and draws the input contours onto the image. DrawFPS \u00b6 This node draws the number of frames per second of onto the top left of input img and outputs the updated image.","title":"Drawing Operations"},{"location":"modules/draw/#bitwiseand","text":"Uses a mask to select and output only the pixels of the input img that are white on the mask input. These pixels are taken and put to an output img , with the rest of the pixels being black.","title":"BitwiseAND"},{"location":"modules/draw/#drawcontours","text":"Takes an image img and draws the input contours onto the image.","title":"DrawContours"},{"location":"modules/draw/#drawfps","text":"This node draws the number of frames per second of onto the top left of input img and outputs the updated image.","title":"DrawFPS"},{"location":"modules/mask/","text":"The Mask module contains nodes used to manipulate masks (which take the form of imgBW ). Masks can be turned into contours using FindContours or filter images using BitwiseAND . These nodes can be found under the opsi-draw tab. Dilate \u00b6 Takes a mask ( imgBW ) and extends all of its borders by the number of pixels defined in the size setting. Commonly used to reduce noise in an image, along with Erode . Erode \u00b6 Takes a mask ( imgBW ) and removes the number of pixels around all its borders as defined in the size setting. Commonly used to reduce noise in an image, along with Dilate . Invert \u00b6 Inverts the mask such that all white areas become black and vise versa, effectively inverting the mask's effect. Join \u00b6 Joins two masks ( imgBW1 and imgBW2 ) such that if a pixel is white on either mask, it is white on the output mask.","title":"Mask Operations"},{"location":"modules/mask/#dilate","text":"Takes a mask ( imgBW ) and extends all of its borders by the number of pixels defined in the size setting. Commonly used to reduce noise in an image, along with Erode .","title":"Dilate"},{"location":"modules/mask/#erode","text":"Takes a mask ( imgBW ) and removes the number of pixels around all its borders as defined in the size setting. Commonly used to reduce noise in an image, along with Dilate .","title":"Erode"},{"location":"modules/mask/#invert","text":"Inverts the mask such that all white areas become black and vise versa, effectively inverting the mask's effect.","title":"Invert"},{"location":"modules/mask/#join","text":"Joins two masks ( imgBW1 and imgBW2 ) such that if a pixel is white on either mask, it is white on the output mask.","title":"Join"},{"location":"modules/nt/","text":"The NetworkTables (NT) module contains nodes used to output data to NetworkTables. NetworkTables is a framework provided by FIRST to easily send data between processors on the robot and to the driver station. These nodes can be found under the opsi-nt tab. Accessing NetworkTables Values \u00b6 To access values from NetworkTables, use the following example code snippets for Java and C++. Replace the values key and table based off of the options in the node, and type based off what type of value you are outputting. E.g. boolean , int , double , etc. Basic accessing of NetworkTables in Java and C++. See the WPILib docs ( Java / C++ ) for more information. Java NetworkTableInstance . getDefault (). getTable ( \"<table>\" ). getEntry ( \"<key>\" ). get < Type > ( < default > ); C++ nt :: NetworkTableInstance :: GetDefault (). GetTable ( \"<table>\" ) -> Get < Type > ( \"<key>\" , < default > ); PutCoordinate \u00b6 Takes a coordinate value and outputs it to NetworkTables. These values can be found under the table defined in the path setting, and under the values [ key ]- x and [ key ]- y . This node will often follow a FindCenter node to output the center found. PutNT \u00b6 The PutNT node puts a value of any type, not just a coordinate, to NetworkTables. This value can be found under the table defined in the path setting, and under the value defined in the key setting. GetNT \u00b6 We are currently working on a node to retrieve values from NetworkTables. You can view the progress on this branch .","title":"NetworkTables"},{"location":"modules/nt/#accessing-networktables-values","text":"To access values from NetworkTables, use the following example code snippets for Java and C++. Replace the values key and table based off of the options in the node, and type based off what type of value you are outputting. E.g. boolean , int , double , etc. Basic accessing of NetworkTables in Java and C++. See the WPILib docs ( Java / C++ ) for more information. Java NetworkTableInstance . getDefault (). getTable ( \"<table>\" ). getEntry ( \"<key>\" ). get < Type > ( < default > ); C++ nt :: NetworkTableInstance :: GetDefault (). GetTable ( \"<table>\" ) -> Get < Type > ( \"<key>\" , < default > );","title":"Accessing NetworkTables Values"},{"location":"modules/nt/#putcoordinate","text":"Takes a coordinate value and outputs it to NetworkTables. These values can be found under the table defined in the path setting, and under the values [ key ]- x and [ key ]- y . This node will often follow a FindCenter node to output the center found.","title":"PutCoordinate"},{"location":"modules/nt/#putnt","text":"The PutNT node puts a value of any type, not just a coordinate, to NetworkTables. This value can be found under the table defined in the path setting, and under the value defined in the key setting.","title":"PutNT"},{"location":"modules/nt/#getnt","text":"We are currently working on a node to retrieve values from NetworkTables. You can view the progress on this branch .","title":"GetNT"},{"location":"modules/videoio/","text":"The VideoIO module contains nodes used to get and output video from the coprocessor OpenSight is being run on. These nodes can be found under the opsi-videoio tab. CameraInput \u00b6 Outputs an img of what a camera sees. The dropdown setting of the node controls the resolution and speed of the camera. The options brightness , contrast , saturation , and exposure control the corresponding settings on the camera video feed. For a quick summary, brightness controls how light the image is. contrast controls the contrast between light and dark on the output of the image. saturation controls how saturated the image is with white colors. exposure controls how long the camera absorbs light for each frame. CameraServer \u00b6 The CameraServer node takes an img input and outputs it to a stream so that it can be viewed on the driver station. You can see the Camera Stream on the Dashboard/Shuffleboard, or you can go to the hooks menu and click opsi.videoio and select the CameraServer.","title":"Video IO"},{"location":"modules/videoio/#camerainput","text":"Outputs an img of what a camera sees. The dropdown setting of the node controls the resolution and speed of the camera. The options brightness , contrast , saturation , and exposure control the corresponding settings on the camera video feed. For a quick summary, brightness controls how light the image is. contrast controls the contrast between light and dark on the output of the image. saturation controls how saturated the image is with white colors. exposure controls how long the camera absorbs light for each frame.","title":"CameraInput"},{"location":"modules/videoio/#cameraserver","text":"The CameraServer node takes an img input and outputs it to a stream so that it can be viewed on the driver station. You can see the Camera Stream on the Dashboard/Shuffleboard, or you can go to the hooks menu and click opsi.videoio and select the CameraServer.","title":"CameraServer"},{"location":"quickstart/getting-started/","text":"Once you have OpenSight installed, you need to access your Raspberry Pi to view the interface. The way to accomplish this will vary based on your setup. If you are on a robot network, simply go to http://opensight.local/ in your browser. Note Currently there is no mechanism for static IPs, however this is a planned feature for the next minor update. Nodetree Interface \u00b6 The nodetree interface is the heart of OpenSight. Here you will see everything you need in order to create a working vision pipeline. Each draggable block is called a \" node \". Every node an instance of a certain \" function \", which defines what it does. For example, seen below is a node with the blur function. Each node has inputs and outputs . You can drag the output of any node into any input of the same type. Each input and output has a name which should give you a good idea what it can connect to. For example, the blur node above inputs any image and also outputs an image. Many nodes have settings . These are options which affect what the function of a node actually does. For example, the radius setting in the blur node affects how strongly the input image should be blurred. Node Menu \u00b6 Different functions are broken up into different modules . Each module is displayed separately in the node menu. Similar nodes are grouped together in modules. For example, nodes pertaining to NetworkTables are kept in opsi-nt, and nodes pertaining to contours can be found in opsi-contours. You can click on a button to create a node of that name. Status Indicator \u00b6 The status indicator is at the bottom right of the nodetree interface. Here, you can see the status of your nodetree. If it has saved and started to run successfully, you will see a green checkmark. If there is an error, an X will be displayed. You can hover over this X to see the error. After modifying the nodetree, the status indicator will turn gray and then run a spinning animation while it is being saved and tested. Settings \u00b6 The settings menu is where the system configuration is located, as compared to node-level configuration. Network Config \u00b6 Static IP Currently, this indicates whether your roboRIO network uses a static IP system (eg. access your roboRIO at 10.TE.AM.2). A proper static IP setting mechanism is planned for update v0.2.0. Server Mode If you aren't on a roboRIO network and want to access NetworkTables, you can enable \"Server\" mode. In Shuffleboard settings you can set the \"Server\" to the IP of your OpenSight instance (eg. opensight.local). If you are on a roboRIO network, this should always be \"Client\". Note Some options, such as system restart options and the updater are only available on the Raspberry Pi or other ARM systems. Viewing the Camera Stream \u00b6 You can view the Camera Stream through a part in OpenSight known as a \"Hook\". Each module can have its own hook. A hook is webpage which allows a module or node to be viewable in some way. Camera Server uses a hook which allows you to view the stream of every camera server by clicking the opsi.videio hook. NetworkTables will also have its own hook soon.","title":"Getting Started"},{"location":"quickstart/getting-started/#nodetree-interface","text":"The nodetree interface is the heart of OpenSight. Here you will see everything you need in order to create a working vision pipeline. Each draggable block is called a \" node \". Every node an instance of a certain \" function \", which defines what it does. For example, seen below is a node with the blur function. Each node has inputs and outputs . You can drag the output of any node into any input of the same type. Each input and output has a name which should give you a good idea what it can connect to. For example, the blur node above inputs any image and also outputs an image. Many nodes have settings . These are options which affect what the function of a node actually does. For example, the radius setting in the blur node affects how strongly the input image should be blurred.","title":"Nodetree Interface"},{"location":"quickstart/getting-started/#node-menu","text":"Different functions are broken up into different modules . Each module is displayed separately in the node menu. Similar nodes are grouped together in modules. For example, nodes pertaining to NetworkTables are kept in opsi-nt, and nodes pertaining to contours can be found in opsi-contours. You can click on a button to create a node of that name.","title":"Node Menu"},{"location":"quickstart/getting-started/#status-indicator","text":"The status indicator is at the bottom right of the nodetree interface. Here, you can see the status of your nodetree. If it has saved and started to run successfully, you will see a green checkmark. If there is an error, an X will be displayed. You can hover over this X to see the error. After modifying the nodetree, the status indicator will turn gray and then run a spinning animation while it is being saved and tested.","title":"Status Indicator"},{"location":"quickstart/getting-started/#settings","text":"The settings menu is where the system configuration is located, as compared to node-level configuration.","title":"Settings"},{"location":"quickstart/getting-started/#network-config","text":"Static IP Currently, this indicates whether your roboRIO network uses a static IP system (eg. access your roboRIO at 10.TE.AM.2). A proper static IP setting mechanism is planned for update v0.2.0. Server Mode If you aren't on a roboRIO network and want to access NetworkTables, you can enable \"Server\" mode. In Shuffleboard settings you can set the \"Server\" to the IP of your OpenSight instance (eg. opensight.local). If you are on a roboRIO network, this should always be \"Client\". Note Some options, such as system restart options and the updater are only available on the Raspberry Pi or other ARM systems.","title":"Network Config"},{"location":"quickstart/getting-started/#viewing-the-camera-stream","text":"You can view the Camera Stream through a part in OpenSight known as a \"Hook\". Each module can have its own hook. A hook is webpage which allows a module or node to be viewable in some way. Camera Server uses a hook which allows you to view the stream of every camera server by clicking the opsi.videio hook. NetworkTables will also have its own hook soon.","title":"Viewing the Camera Stream"},{"location":"quickstart/hardware/","text":"OpenSight primarily targets the Raspberry Pi, however, it can be run on numerous different setups. OpenSight supports all cameras. If you have any compatibility issues with any coprocessor or camera, come to our Discord . Hardware Configurations \u00b6 The Raspberry Pi (and other coprocessors) require 5V, so if you are using OpenSight on a robot, you will need to step down from 12V to 5V. All configurations include an appropriate step down module. Here are some recommended configurations at different price points. Each configuration inculdes a either typical or estimated performance. Any configurations with an estimated performance will be marked with a *. If you have any of these configurations and want to submit performance figures, we would greatly appreciate it! Most product links are Amazon links for consistency, however many of these parts can be found cheaper from their manufactuer or other websites. Warning Please note that we have not tested all of these items. We are not specifically recommending any of these products. We are not liable for any non-working configurations or for any damage you do to any components. OscarEye \u00b6 Raspberry Pi 4 85 FPS @ 320x240 \u00b6 Item Link Price Raspberry Pi 4 (1GB RAM) https://amazon.com/dp/B07TD43PDZ $41.99 Raspberry Pi 4 Case https://amazon.com/dp/B07W3ZMVP1 $8.99 Arducam 5 https://amazon.com/dp/B012V1HEP4 $12.99 16GB MicroSD Card https://amazon.com/dp/B073K14CVB $5.79 HOMREE DC 12V to DC 5V (USB-C) https://amazon.com/dp/B07ZQB6S3L $10.90 Total $80.66 General Setup \u00b6 To connect any coprocessor to the robot, you will need an Ethernet cable . Plug the Ethernet cable into the extra port on the radio to connect your coprocessor to the robot. Using a Pi Cam \u00b6 If you are using a Pi Cam (such as the Arducam 5) you will need to connect your Pi Cam to the port boxed in PURPLE on the image. Do not connect it to the port boxed in BLUE on the image. LEDs \u00b6 You can control LEDs either directly from the robot using the PCM, or by using GPIO (GPIO module coming soon). PCM \u00b6 If you want to use PCM, make sure your PCM is running 12V, not 24V . Once you plug your LEDs into the PCM, you can control them from the Robot Code in the same way you would control a regular solenoid ( Java / C++ ). GPIO \u00b6 If you plan on using GPIO, you will need a relay switch of some sort. Keep in mind that the Raspberry Pi's GPIO only outputs 3.3V. Here is a relay switch which has been successfully tested with a Raspberry Pi 4. Also, most LEDs run off of 12V. Here is a diagram of how to properly set up LED control from the Pi. Here is a diagram of how to properly set this up:","title":"Hardware"},{"location":"quickstart/hardware/#hardware-configurations","text":"The Raspberry Pi (and other coprocessors) require 5V, so if you are using OpenSight on a robot, you will need to step down from 12V to 5V. All configurations include an appropriate step down module. Here are some recommended configurations at different price points. Each configuration inculdes a either typical or estimated performance. Any configurations with an estimated performance will be marked with a *. If you have any of these configurations and want to submit performance figures, we would greatly appreciate it! Most product links are Amazon links for consistency, however many of these parts can be found cheaper from their manufactuer or other websites. Warning Please note that we have not tested all of these items. We are not specifically recommending any of these products. We are not liable for any non-working configurations or for any damage you do to any components.","title":"Hardware Configurations"},{"location":"quickstart/hardware/#oscareye","text":"","title":"OscarEye"},{"location":"quickstart/hardware/#raspberry-pi-4-85-fps-320x240","text":"Item Link Price Raspberry Pi 4 (1GB RAM) https://amazon.com/dp/B07TD43PDZ $41.99 Raspberry Pi 4 Case https://amazon.com/dp/B07W3ZMVP1 $8.99 Arducam 5 https://amazon.com/dp/B012V1HEP4 $12.99 16GB MicroSD Card https://amazon.com/dp/B073K14CVB $5.79 HOMREE DC 12V to DC 5V (USB-C) https://amazon.com/dp/B07ZQB6S3L $10.90 Total $80.66","title":"Raspberry Pi 4 85 FPS @ 320x240"},{"location":"quickstart/hardware/#general-setup","text":"To connect any coprocessor to the robot, you will need an Ethernet cable . Plug the Ethernet cable into the extra port on the radio to connect your coprocessor to the robot.","title":"General Setup"},{"location":"quickstart/hardware/#using-a-pi-cam","text":"If you are using a Pi Cam (such as the Arducam 5) you will need to connect your Pi Cam to the port boxed in PURPLE on the image. Do not connect it to the port boxed in BLUE on the image.","title":"Using a Pi Cam"},{"location":"quickstart/hardware/#leds","text":"You can control LEDs either directly from the robot using the PCM, or by using GPIO (GPIO module coming soon).","title":"LEDs"},{"location":"quickstart/hardware/#pcm","text":"If you want to use PCM, make sure your PCM is running 12V, not 24V . Once you plug your LEDs into the PCM, you can control them from the Robot Code in the same way you would control a regular solenoid ( Java / C++ ).","title":"PCM"},{"location":"quickstart/hardware/#gpio","text":"If you plan on using GPIO, you will need a relay switch of some sort. Keep in mind that the Raspberry Pi's GPIO only outputs 3.3V. Here is a relay switch which has been successfully tested with a Raspberry Pi 4. Also, most LEDs run off of 12V. Here is a diagram of how to properly set up LED control from the Pi. Here is a diagram of how to properly set this up:","title":"GPIO"},{"location":"quickstart/installation/","text":"Depending on what system you are trying to install on, the process may differ. If you are installing on Raspberry Pi, go to the appropriate section . If you are trying to install OpenSight on a different coprocessor, you can find the process here . Currently, OpenSight is only directly supported on Linux. It should still work on Windows or MacOS, however the process to do so is far too elaborate to explain here (requries compiling multiple Python libraries). To install on any other Linux system, you can follow the other systems installation . Installing on Raspberry Pi \u00b6 Note This is only required on the first installation of the image. After that, you can use the upgrader in order to upgrade your installation. Any exceptions to this will be plainly noted on the releases page. Insert your Raspberry Pi's Micro SD card into your computer. Please note that you need a Micro SD card with at least 4GB. All data on the SD card will be erased during the installation process . Download the latest Raspberry Pi image file from the releases page . Install a disk imaging software. The simplest imaging software is balenaEtcher . Click \"Select an Image\" and select the .zip file image you downloaded in step 1. For the drive, select your Micro SD card. Click \"Flash\". After this finishes, you can put your Micro SD card back into your Pi. OpenSight should now be running! You can continue to the Getting Started page. If you have any issues, please join our Discord , let us know the issue in the #help channel and we'll help you troubleshoot. Installing on Debian ARM systems \u00b6 You must install Debian on your coprocessor in order to continue with this section. If you are running a Debian ARM based system, you can use the same packages generated for the Raspberry Pi. If the output of dpkg-architecture -q DEB_BUILD_ARCH is armhf , you can use the following set of instructions. Types of systems that meet this requirement would be: Raspberry Pi (without erasing anything on your current Raspbian installation) Many coprocessors Here are the differences between using the packages and installing with the regular Linux script: Automatic startup Installed system-wide Can use the upgrader Run these commands to do so: sudo apt update sudo apt install -y curl git jq mkdir /tmp/opsi ; cd /tmp/opsi url = \" $( curl https://api.github.com/repos/opensight-cv/packages/releases/latest | jq -r '.[\"assets\"][][\"browser_download_url\"]' | grep -v with ) \" curl -LO $url mkdir -p packages tar xf opsi-packages-*.tar.gz -C packages suggests = $( dpkg-deb -I \"/packages/deps/opensight_\" * \".deb\" | grep Suggests | sed -e 's/ Suggests: //' -e 's/:.*$//g' -e 's/\\n/ /g' -e 's/(/@/g' -e 's/)/@/g' -e 's/ @\\([^@]*\\)@//g' -e \"s/,//g\" ) sudo apt install -y ./packages/deps/*.deb $suggests rm -rf /tmp/opsi/ reboot Once your coprocessor restats, OpenSight should now be running! You can continue to the Getting Started page. Installing on other systems \u00b6 If you are running a non-Debian derivative system (eg. anything other than Debian, Ubuntu, or Mint), you will need to install your distribution's version of the following: build-essential git curl python3.7 python3-dev python3.7-dev python3-pip python3-venv Once you have done this, or if you are running a Debian derivative, simply run this command to create an OpenSight instance in your current directory: curl -O https://opensight-cv.github.io/install-opsi.sh chmod +x install-opsi.sh ./install-opsi.sh or curl https://opensight-cv.github.io/install-opsi.sh | bash After you do so, you should be able to cd into the opensight folder and run the script: ./run.sh . Warning Please only run the one line command if you understand the security implications of it. We recommend you inspect the installation script to ensure it is safe.","title":"Installation"},{"location":"quickstart/installation/#installing-on-raspberry-pi","text":"Note This is only required on the first installation of the image. After that, you can use the upgrader in order to upgrade your installation. Any exceptions to this will be plainly noted on the releases page. Insert your Raspberry Pi's Micro SD card into your computer. Please note that you need a Micro SD card with at least 4GB. All data on the SD card will be erased during the installation process . Download the latest Raspberry Pi image file from the releases page . Install a disk imaging software. The simplest imaging software is balenaEtcher . Click \"Select an Image\" and select the .zip file image you downloaded in step 1. For the drive, select your Micro SD card. Click \"Flash\". After this finishes, you can put your Micro SD card back into your Pi. OpenSight should now be running! You can continue to the Getting Started page. If you have any issues, please join our Discord , let us know the issue in the #help channel and we'll help you troubleshoot.","title":"Installing on Raspberry Pi"},{"location":"quickstart/installation/#installing-on-debian-arm-systems","text":"You must install Debian on your coprocessor in order to continue with this section. If you are running a Debian ARM based system, you can use the same packages generated for the Raspberry Pi. If the output of dpkg-architecture -q DEB_BUILD_ARCH is armhf , you can use the following set of instructions. Types of systems that meet this requirement would be: Raspberry Pi (without erasing anything on your current Raspbian installation) Many coprocessors Here are the differences between using the packages and installing with the regular Linux script: Automatic startup Installed system-wide Can use the upgrader Run these commands to do so: sudo apt update sudo apt install -y curl git jq mkdir /tmp/opsi ; cd /tmp/opsi url = \" $( curl https://api.github.com/repos/opensight-cv/packages/releases/latest | jq -r '.[\"assets\"][][\"browser_download_url\"]' | grep -v with ) \" curl -LO $url mkdir -p packages tar xf opsi-packages-*.tar.gz -C packages suggests = $( dpkg-deb -I \"/packages/deps/opensight_\" * \".deb\" | grep Suggests | sed -e 's/ Suggests: //' -e 's/:.*$//g' -e 's/\\n/ /g' -e 's/(/@/g' -e 's/)/@/g' -e 's/ @\\([^@]*\\)@//g' -e \"s/,//g\" ) sudo apt install -y ./packages/deps/*.deb $suggests rm -rf /tmp/opsi/ reboot Once your coprocessor restats, OpenSight should now be running! You can continue to the Getting Started page.","title":"Installing on Debian ARM systems"},{"location":"quickstart/installation/#installing-on-other-systems","text":"If you are running a non-Debian derivative system (eg. anything other than Debian, Ubuntu, or Mint), you will need to install your distribution's version of the following: build-essential git curl python3.7 python3-dev python3.7-dev python3-pip python3-venv Once you have done this, or if you are running a Debian derivative, simply run this command to create an OpenSight instance in your current directory: curl -O https://opensight-cv.github.io/install-opsi.sh chmod +x install-opsi.sh ./install-opsi.sh or curl https://opensight-cv.github.io/install-opsi.sh | bash After you do so, you should be able to cd into the opensight folder and run the script: ./run.sh . Warning Please only run the one line command if you understand the security implications of it. We recommend you inspect the installation script to ensure it is safe.","title":"Installing on other systems"},{"location":"quickstart/upgrading/","text":"Depending on which version of OpenSight you are using, you may or may not be able to use the upgrader. If you installed using the Raspberry Pi image or the Debian ARM systems (packages) method you are able to use the upgrader. The current version number is viewable in the bottom left of the settings page. Upgrading on Raspberry Pi \u00b6 If you are on Raspberry Pi, you can do the following to update your OpenSight installation: Download the with dependencies tar.gz file on the latest release page from the packages repository. Go to the settings page OpenSight instance. Drag and drop the file into the \"Update\" box, or browse and select the file. Click the \"Update\" button. Your Raspberry Pi should now start updating. This may take multiple minutes. Once you are able to access OpenSight again, updating should be finished. If OpenSight is not accessible after approximately 10 minutes, either reflash the image or join our Discord for support. Info Technical Implications : The with dependencies tar.gz file contains updates for system packages as well the OpenSight packages. This ensures that any dependencies of OpenSight are upgraded along with OpenSight, and no manual upgrading is required. Upgrading on other Debian systems \u00b6 If you are on a different ARM Debian system, you can do the following to update your OpenSight installation: Download the normal tar.gz file (eg. not the with-dependencies file) on the latest release page from the packages repository. Go to the settings page OpenSight instance. Drag and drop the file into the \"Update\" box, or browse and select the file. Click the \"Update\" button. If OpenSight is not accessible after approximately 10 minutes, either reinstall OpenSight manually or join our Discord for support.","title":"Upgrading"},{"location":"quickstart/upgrading/#upgrading-on-raspberry-pi","text":"If you are on Raspberry Pi, you can do the following to update your OpenSight installation: Download the with dependencies tar.gz file on the latest release page from the packages repository. Go to the settings page OpenSight instance. Drag and drop the file into the \"Update\" box, or browse and select the file. Click the \"Update\" button. Your Raspberry Pi should now start updating. This may take multiple minutes. Once you are able to access OpenSight again, updating should be finished. If OpenSight is not accessible after approximately 10 minutes, either reflash the image or join our Discord for support. Info Technical Implications : The with dependencies tar.gz file contains updates for system packages as well the OpenSight packages. This ensures that any dependencies of OpenSight are upgraded along with OpenSight, and no manual upgrading is required.","title":"Upgrading on Raspberry Pi"},{"location":"quickstart/upgrading/#upgrading-on-other-debian-systems","text":"If you are on a different ARM Debian system, you can do the following to update your OpenSight installation: Download the normal tar.gz file (eg. not the with-dependencies file) on the latest release page from the packages repository. Go to the settings page OpenSight instance. Drag and drop the file into the \"Update\" box, or browse and select the file. Click the \"Update\" button. If OpenSight is not accessible after approximately 10 minutes, either reinstall OpenSight manually or join our Discord for support.","title":"Upgrading on other Debian systems"}]}